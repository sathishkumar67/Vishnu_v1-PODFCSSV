{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe7afad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PODFCSSV'...\n",
      "remote: Enumerating objects: 83, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
      "remote: Total 83 (delta 22), reused 64 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (83/83), 897.43 KiB | 21.89 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sathishkumar67/PODFCSSV.git\n",
    "!mv PODFCSSV/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbbc4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "for item in os.listdir(current_path):\n",
    "    item_path = os.path.join(current_path, item)\n",
    "\n",
    "    if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "        os.remove(item_path)\n",
    "    elif os.path.isdir(item_path):\n",
    "        shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cce147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "def setup_cifar100_continual_federated(\n",
    "    root='./data', \n",
    "    num_clients=2,      # CHANGED: Set to 2 Clients\n",
    "    num_tasks=2,        # CHANGED: Set to 2 Tasks (100 classes / 2 tasks = 50 classes per task)\n",
    "    alpha=0.5, \n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads CIFAR-100 and creates a JSON file mapping:\n",
    "    Client ID -> Task ID -> List of Image Indices\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Data Split: {num_clients} Clients, {num_tasks} Tasks (50 classes/task), Alpha={alpha}\")\n",
    "    \n",
    "    # 1. Fix Seeds for Reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # 2. Download CIFAR-100\n",
    "    # Ensure data directory exists\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    \n",
    "    transform = transforms.ToTensor()\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root=root, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root=root, train=False, download=True, transform=transform)\n",
    "    \n",
    "    targets = np.array(train_dataset.targets)\n",
    "    classes = np.arange(100)\n",
    "    \n",
    "    # 3. Define Tasks (Class Incremental)\n",
    "    # With num_tasks=2, this creates:\n",
    "    # Task 0: Classes 0-49\n",
    "    # Task 1: Classes 50-99\n",
    "    classes_per_task = 100 // num_tasks\n",
    "    task_splits = {t: classes[t*classes_per_task : (t+1)*classes_per_task] for t in range(num_tasks)}\n",
    "    \n",
    "    # Structure to save: client_data[client_id][task_id] = [indices]\n",
    "    client_data = {cid: {tid: [] for tid in range(num_tasks)} for cid in range(num_clients)}\n",
    "    test_data = {tid: [] for tid in range(num_tasks)}\n",
    "\n",
    "    # 4. Perform Dirichlet Split per Task\n",
    "    for task_id, task_classes in task_splits.items():\n",
    "        print(f\"Processing Task {task_id} (Classes {task_classes[0]}-{task_classes[-1]})...\")\n",
    "        \n",
    "        # Save Test Data for this task (Global test set for evaluation)\n",
    "        test_indices = [i for i, t in enumerate(test_dataset.targets) if t in task_classes]\n",
    "        test_data[task_id] = test_indices\n",
    "\n",
    "        # Split Training Data\n",
    "        for c in task_classes:\n",
    "            # Get all indices for this specific class\n",
    "            idx_k = np.where(targets == c)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            \n",
    "            # Generate Dirichlet distribution for this class across clients\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            \n",
    "            # Normalize proportions to strictly sum to 1 (handling floating point issues)\n",
    "            proportions = proportions / proportions.sum()\n",
    "            \n",
    "            # Calculate split points based on proportions\n",
    "            # Logic: Cumulative sum of proportions * total items -> cast to int for indices\n",
    "            split_points = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "            \n",
    "            # Split the indices\n",
    "            idx_batch = np.split(idx_k, split_points)\n",
    "            \n",
    "            # Assign to clients\n",
    "            for client_id in range(num_clients):\n",
    "                client_data[client_id][task_id].extend(idx_batch[client_id].tolist())\n",
    "\n",
    "    # 5. Save Metadata\n",
    "    save_path = os.path.join(root, 'federated_splits.json')\n",
    "    meta_data = {\n",
    "        'client_data': client_data,\n",
    "        'test_data': test_data,\n",
    "        'task_config': {t: task_splits[t].tolist() for t in task_splits}\n",
    "    }\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(meta_data, f)\n",
    "        \n",
    "    print(f\"âœ… Data preparation complete. Splits saved to {save_path}\")\n",
    "    \n",
    "    # Validation Print\n",
    "    print(\"\\n--- Split Statistics ---\")\n",
    "    for cid in range(num_clients):\n",
    "        print(f\"Client {cid}:\")\n",
    "        for tid in range(num_tasks):\n",
    "            count = len(client_data[cid][tid])\n",
    "            print(f\"  Task {tid}: {count} samples (Classes {task_splits[tid][0]}-{task_splits[tid][-1]})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_cifar100_continual_federated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8e973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
